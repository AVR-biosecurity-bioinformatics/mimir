/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    mimir Nextflow config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Default config options for all compute environments
----------------------------------------------------------------------------------------
*/

// Global default params, used in configs

params {

    help                        = null

    slurm_account               = null                              // account to use on SLURM

    entrez_key                  = null                              // user-provided ENTREZ key to speed up NCBI API queries

    //// taxon options
    target_taxon                = null                              // target taxon: either NCBI UID (ie. integer; preferred), or scientific taxon name; must be a single value; string
    target_rank                 = null                              // taxonomic rank of target_taxon to reduce ambiguity when matching; recommended if NCBI UID not provided to --target_taxon; must be single value; string
    target_list                 = null                              // list of target taxa and their ranks, as a comma-delimited file (.csv); one row per taxon; path
    input_chunk_size            = 10000                             // maximum number of sequences to pass through each single filtering process; integer 
    key_species_list            = null                              // list of key species names to validate the database; path (.txt file)

    //// outgroup options
    // outgroups                   = null                              // list of taxa to fetch outgroup sequences from
    // outgroup_chunk_rank         = 'family'                          // taxonomic rank to 
    // outgroup_size               = null                              // number of sequences to return per outgroup chunk
    // outgroup_genetic_codes      = null                              // list of genetic codes to use for each outgroup, see Biostrings::GENETIC_CODE_TABLE

    //// marker options
    marker                      = null                              // marker gene to extract; string (currently can only be 'COI')

    //// internal sequences
    internal_seqs               = null                              // path to .fasta of high-quality internal sequences
    // internal_taxfilter          = false                             // retain only internal sequences from taxa given in `--target_taxa`; boolean

    //// database options
    use_genbank                 = true                              // use NCBI GenBank database; boolean
    use_bold                    = true                              // use BOLD; boolean
    use_mito                    = true                              // use NCBI mitochondrial genomes (for mitochrondial loci only); boolean
    min_length_input            = 100                               // minimum length of record sequences to import, internal and external; doesn't affect mitogenomes; integer
    max_length_input            = 2000                              // maximum length of record sequences to import, internal and external; doesn't affect mitogenomes; integer
    genbank_fetch_size          = 2000                              // maximum number of GenBank sequences to fetch within a single process
    bold_db_path                = null                              // path to manually downloaded BOLD database .tar.gz file; string (path)
        //// NOTE: Above can be path to directory containing compressed or uncompressed database, or a path to the compressed database file
    bold_db_url                 = null                              // manually generated link to the current BOLD database; string (URL)
        //// NOTE: URLs must be supplied on bash command line in double quotes to avoid '&' causing issues
        //// NOTE: if both bold_db_path and bold_db_url are given, the pipeline will check for valid existing files before attemping to download
    bold_idmethod_filter        = false                             // removes BOLD sequences with an "identification_method" value that implies BOLD was used to classify sequences

    ///// filtering options
    remove_ambiguous            = true                              // remove record sequences with one or more ambiguous/degenerate bases (eg. N, R, Y etc.); boolean
    remove_unclassified         = 'all_ranks'                       // remove record sequences with "Unclassified" for any (strict), just terminal (eg. 'species', 'genus + species' etc.; moderate), or all (lenient) taxonomic ranks; string ('any_ranks'/'all_ranks'/'terminal'/'none')
    placeholder_as_unclassified = true                              // replace placeholder species names, eg. "Genus sp. XYZ", with "Unclassified" and process according to '--remove_unclassified' (detects period characters); boolean
    digits_as_unclassified      = true                              // replace taxonomic names that contain digits with "Unclassified" and process according to '--remove_unclassified'; boolean

    ///// full PHMM options
    phmm_max_evalue             = '1e-10'                           // maximum E-value for a full PHMM hit to be retained; scientific number
    phmm_min_score              = 50                                // minimum bit score for a full PHMM hit to be retained; integer
    phmm_max_hits               = 1                                 // maximum number of allowed full PHMM hits within a record sequence; integer
    phmm_min_acc                = 0.85                              // minimum 'mean posterior probability of aligned residues' for full PHMM hits (see HMMER documentation); float (0-1)
    phmm_max_gap                = 5                                 // maximum number of amino acid residues (or equivalent) missing from full PHMM and record sequence at the start or end of a record sequence; integer
    phmm_min_cov                = 0.1                               // minimum coverage of the full PHMM for a record sequence to be retained; float (0-1.0)
    phmm_min_length             = 100                               // minumum length (in bp) for record sequences after being trimmed to the full PHMM; integer 

    ///// amplicon options
    primer_fwd                  = null                              // forward primer sequence (5'-3') to define the barcode amplicon for trimming and/or filtering; string
    primer_rev                  = null                              // reverse primer sequence (5'-3') to define the barcode amplicon for trimming and/or filtering; string
    trim_to_amplicon            = false                             // trim record sequences to primer-amplified region using trimmed PHMM; boolean
    pad_fwd                     = 0                                 // bp to pad from the fwd primer when trimming; integer >= 0; ignored when `--remove_primers true`
    pad_rev                     = 0                                 // bp to pad from the rev primer when trimming; integer >= 0; ignored when `--remove_primers true`
    remove_primers              = false                             // remove primer-binding sequences from record sequences (must be used with `--trim_to_primers`); boolean
    amplicon_min_cov            = 0.95                              // minimum coverage of the trimmed PHMM for a record sequence to be retained; note that 100% is usually unlikely even for full-length sequences; float (0-1.0)
        //// above allows records to be filtered even if `--trim_to_primers false`, based on how much of the trimmed PHMM region is present in the record
    amplicon_min_length         = 100                               // minimum length (in bp) of a record sequence after being trimmed to the primer-trimmed PHMM; integer

    ///// clustering options
    cluster_rank                = 'genus'                           // taxonomic rank(s) to check clusters, can be a comma-delimited list (without spaces); string
    cluster_threshold           = 0.97                              // OTU clustering threshold; number (0-1)
    cluster_confidence          = 0.8                               // taxonomic proportion within a cluster to reject remaining sequences; number (0-1)

    split_rank_chunk            = 500000                            // number of sequences to send to each instance of `SPLIT_GENUS` at once; integer

    dist_threshold              = 0.05                              // intraspecific (Jukes-Cantor-corrected) genetic distance from the central sequence above which sequences will be removed as outliers; number (0-1) 

    //// final sequence selection options
    max_group_size              = 1                                 // maximum number of sequences to retain per taxonomic ID; integer
    selection_method            = 'length'                          // method to prune sequences, "length" (smallest first) or "random"; string

    //// final formatting options
    add_root                    = true                              // add "Root" to reformatted taxonomic hierarchy in sequence headers; boolean
    aligned_output              = false                             // final database is an aligned .fasta; boolean
    compressed_output           = false                             // final database file is gzip compressed; boolean

    ///// model training options
    train_idtaxa                = false                             // whether to train an IDTAXA model from the final database; boolean
    idtaxa_max_group_size       = 10                                // number of sequences to retain, per species, before training IDTAXA model; integer
    idtaxa_max_iterations       = 3                                 // number of times to iteratively run the IDTAXA training process; integer
    idtaxa_allow_group_removal  = true                              // whether or not IDTAXA should be allowed to remove taxonomic groups from the database when training; boolean

    ///// debugging options
    debug_mode                  = false                             // display some debug information in the pipeline output; boolean
    rdata                       = false                             // save all data/objects from process-level R sessions as .RData files in work dir; boolean
    save_intermediate           = false                             // save combined .fasta files after each filtering step (in ./output/results); boolean
    save_input                  = false                             // save re-formatted input sequences as a combined .fasta file (in ./output/results); boolean

    ///// Max resource options per task/process
    // Defaults only, expecting to be overwritten
    max_memory                  = '128.GB'
    max_cpus                    = 16
    max_time                    = '240.h'

}


validation {

    failUnrecognisedParams      = false                             // run will fail if unrecognised parameters are given, not just give a warning

}


profiles {

    basc_slurm {
        process.executor                = 'slurm'
        process.queue                   = 'batch,shortrun'
        params.slurm_account            = 'ngdsi'
        process.clusterOptions          = "--account $params.slurm_account"
        params.max_memory               = '512.GB'
        params.max_time                 = '168.h'
        params.max_cpus                 = 48
        executor.queueSize              = 5000
        // executor.pollInterval           = '10 sec'
        // executor.submitRateLimit        = '5 sec'
        process.module                  = 'shifter' // this runs 'module load shifter' at the start of each process job
        shifter.enabled                 = true
    }       
    test { /// this profile should always be specified1 last to force the minimal resource requirements
        params.max_memory               = '2.GB'
        params.max_time                 = '10.m'
        params.max_cpus                 = 1
        params.phmm_model               = 'assets/folmer_fullength_model.rds'
    }       
    debug {     
        params.rdata                    = true
        params.save_intermediate        = true
        params.debug_mode               = true
    }      
    error1_backoff {
        process.errorStrategy           = {
            if ( task.exitStatus in ((130..145) + 104) ) {
                'retry'
            } else if ( task.exitStatus == 1 ) {
                sleep(Math.pow(2, task.attempt) * 2 as long); return 'retry'
            } else {
                'finish'
            }
        }
    }  
    apptainer {         
        apptainer.enabled               = true
        apptainer.autoMounts            = true
        charliecloud.enabled            = false
        conda.enabled                   = false
        docker.enabled                  = false
        podman.enabled                  = false        
        shifter.enabled                 = false
        singularity.enabled             = false
    }           
    docker {            
        apptainer.enabled               = false
        charliecloud.enabled            = false
        conda.enabled                   = false
        docker.enabled                  = true
        docker.runOptions               = '-u $(id -u):$(id -g)'
        podman.enabled                  = false        
        shifter.enabled                 = false
        singularity.enabled             = false
    }           
    podman {            
        apptainer.enabled               = false
        charliecloud.enabled            = false
        conda.enabled                   = false
        docker.enabled                  = false
        podman.enabled                  = true        
        shifter.enabled                 = false
        singularity.enabled             = false
    }           
    shifter {           
        apptainer.enabled               = false
        charliecloud.enabled            = false
        conda.enabled                   = false
        docker.enabled                  = false
        podman.enabled                  = false        
        shifter.enabled                 = true
        singularity.enabled             = false
    }           
    singularity {           
        apptainer.enabled               = false
        charliecloud.enabled            = false
        conda.enabled                   = false
        docker.enabled                  = false
        podman.enabled                  = false        
        shifter.enabled                 = false
        singularity.enabled             = true
        singularity.autoMounts          = true
    }


}


process {

    // error handling
    errorStrategy = { task.exitStatus in ((130..145) + 104) ? 'retry' : 'finish' }
    maxRetries    = 3
    maxErrors     = '-1'
    withLabel: error_retry {
        errorStrategy = 'retry'
        maxRetries    = 3
    }

    // process resources
    withName: ALIGN_BATCH {
        cpus    = { cpu_scale  ( 4,     task.attempt ) }
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 2.h,   task.attempt ) }
    }
    withName: ALIGN_CORE {
        cpus    = { cpu_scale  ( 16,    task.attempt ) }
        memory  = { mem_scale  ( 16.GB, task.attempt ) }
        time    = { time_scale ( 4.h,   task.attempt ) }
    }
    withName: ALIGN_OTHER {
        cpus    = { cpu_scale  ( 16,    task.attempt ) }
        memory  = { mem_scale  ( 16.GB, task.attempt ) }
        time    = { time_scale ( 8.h,   task.attempt ) }
    }
    withName: ALIGN_PRIMERS_TO_SEED {
        cpus    = { cpu_scale  ( 1,     task.attempt ) }
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: ALIGN_SUBSAMPLE {
        cpus    = { cpu_scale  ( 8,    task.attempt ) }
        memory  = { mem_scale  ( 8.GB, task.attempt ) }
        time    = { time_scale ( 1.h,   task.attempt ) }
    }
    withName: BUILD_AMPLICON_HMM {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 10.m,  task.attempt ) }
    }
    withName: CHECK_KEY_SPECIES {
        cpus    = 1
        memory  = { mem_scale  ( 16.GB, task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: CLUSTER_SEQUENCES {
        cpus    = { cpu_scale  ( 4,     task.attempt ) }
        memory  = { mem_scale  ( 16.GB, task.attempt ) }
        time    = { time_scale ( 2.h,   task.attempt ) }
    }
    withName: COMBINE_CHUNKS {
        cpus    = { cpu_scale  ( 1,     task.attempt ) }
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: ESTIMATE_THRESHOLDS {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: EXTRACT_BOLD {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: FASTA_TO_TABLE {
        cpus    = 1
        memory  = { mem_scale  ( 2.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: FETCH_GENBANK {
        cpus    = 1
        memory  = { mem_scale  ( 1.GB,  task.attempt ) }
        time    = { time_scale ( 20.m,  task.attempt ) }
    }
    withName: FILTER_AMBIGUOUS {
        cpus    = 1
        memory  = { mem_scale  ( 2.GB,  task.attempt ) }
        time    = { time_scale ( 10.m,  task.attempt ) }
    }
    withName: FILTER_DUPLICATES {
        cpus    = 1
        memory  = { mem_scale  ( 2.GB,  task.attempt ) }
        time    = { time_scale ( 10.m,  task.attempt ) }
    }
    withName: FILTER_PHMM_FULL {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: FILTER_PHMM_AMPLICON {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: FILTER_REDUNDANT {
        cpus    = { cpu_scale  ( 1,     task.attempt ) }
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 1.h,  task.attempt ) }
    }
    withName: FILTER_SEQ_OUTLIERS {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 1.h,  task.attempt ) }
    }
    withName: FILTER_TAX_OUTLIERS {
        cpus    = 1
        memory  = { mem_scale  ( 16.GB, task.attempt ) }
        time    = { time_scale ( 2.h,   task.attempt ) }
    }
    withName: FILTER_UNCLASSIFIED {
        cpus    = 1
        memory  = { mem_scale  ( 2.GB,  task.attempt ) }
        time    = { time_scale ( 10.m,  task.attempt ) }
    }
    withName: FORMAT_OUTPUT {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 1.h,   task.attempt ) }
    }
    withName: GET_BOLD_DATABASE {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: GET_CORE_SEQUENCES {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: GET_NCBI_TAXONOMY {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: HMMSEARCH_FULL {
        cpus    = { cpu_scale  ( 1,     task.attempt ) }
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 10.m,  task.attempt ) }
    }
    withName: HMMSEARCH_AMPLICON {
        cpus    = { cpu_scale  ( 1,     task.attempt ) }
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 10.m,  task.attempt ) }
    }
    withName: IMPORT_INTERNAL {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: JOIN_SOURCES_FATES {
        cpus    = 1
        memory  = { mem_scale  ( 16.GB, task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: MAKE_GENCODES {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: MAKE_LINEAGEPARENTS {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: MAKE_NAMES {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: MAKE_NODES {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: MAKE_RANKEDLINEAGE_NONAME {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: MAKE_RANKEDLINEAGE {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: MAKE_SYNONYMS {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: MAKE_TAXIDNAMERANK {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: MAKE_TAXIDNAMES {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: MATCH_BOLD {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: MERGE_BOLD {
        cpus    = 1
        memory  = { mem_scale  ( 2.GB,  task.attempt ) }
        time    = { time_scale ( 10.m,  task.attempt ) }
    }
    withName: MERGE_SPLITS {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: PARSE_MARKER {
        cpus    = 1
        memory  = { mem_scale  ( 2.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: PARSE_TARGETS {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: PROCESS_PRIMERS {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 10.m,  task.attempt ) }
    }
    withName: QUERY_GENBANK {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 2.h,   task.attempt ) }
    }
    withName: RENAME_GENBANK {
        cpus    = 1
        memory  = { mem_scale  ( 4.GB,  task.attempt ) }
        time    = { time_scale ( 5.m,   task.attempt ) }
    }
    withName: SELECT_FINAL_SEQUENCES {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: SEQUENCE_TRACKER {
        cpus    = 1
        memory  = { mem_scale  ( 16.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: SORT_BY_LINEAGE {
        cpus    = 1
        memory  = { mem_scale  ( 2.GB,  task.attempt ) }
        // dynamic time in process .nf file
    }
    withName: SPLIT_BY_RANK {
        cpus    = 1
        memory  = { mem_scale  ( 2.GB,  task.attempt ) }
        // dynamic time in process .nf file
    }
    withName: STOCKHOLM_TO_FASTA {
        cpus    = 1
        memory  = { mem_scale  ( 2.GB,  task.attempt ) }
        time    = { time_scale ( 10.m,  task.attempt ) }
    }
    withName: SUBSAMPLE_RECORDS {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 1.h,  task.attempt ) }
    }
    withName: SUMMARISE_SUBSAMPLES {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 1.h,  task.attempt ) }
    }
    withName: TRAIN_IDTAXA {
        cpus    = 1
        memory  = { mem_scale  ( 64.GB, task.attempt ) }
        time    = { time_scale ( 12.h,  task.attempt ) }
    }
    withName: TRANSLATE_SEQUENCES {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: TRIM_HMM_SEED {
        cpus    = 1
        memory  = { mem_scale  ( 8.GB,  task.attempt ) }
        time    = { time_scale ( 30.m,  task.attempt ) }
    }
    withName: VALIDATE_KEY_SPECIES {
        cpus    = 1
        memory  = { mem_scale  ( 16.GB, task.attempt ) }
        time    = { time_scale ( 4.h,   task.attempt ) }
    }

}


plugins {
    id 'nf-schema@2.0.0'                            // create schema to validate sample sheets and pipeline parameters
}


report {
    enabled             = true
    overwrite           = true
    file                = "output/run_info/report.html"
}


trace {
    enabled             = true
    overwrite           = true
    file                = "output/run_info/trace.tsv"
}


dag {
    enabled             = true
    overwrite           = true
    file                = "output/run_info/dag.html"
    verbose             = true
}


timeline {
    enabled             = true
    overwrite           = true
    file                = "output/run_info/timeline.html"
}


// Function to ensure that resource requirements don't go beyond a maximum limit
// from: https://github.com/nf-core/tools/blob/99961bedab1518f592668727a4d692c4ddf3c336/nf_core/pipeline-template/nextflow.config#L206-L237
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}

// functions to increase resources in proportion to task attempts while staying within max limits
def cpu_scale ( obj, iteration ) {
    return check_max( obj * iteration , 'cpus' )
}

def mem_scale ( obj, iteration ) {
    return check_max( obj * iteration , 'memory' )
}

def time_scale ( obj, iteration ) {
    return check_max( obj * iteration , 'time' )
}

